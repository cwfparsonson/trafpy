{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Benchmark Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['/rdata/ong/trafpy/management/jobcentric/jobcentric_prototyping_k_4_L_2_n_16_chancap1_numchans1_mldat3e3_bidirectional_slotsize_10.0_testbed_data/benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import trafpy\n",
    "import trafpy.generator as tpg\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "JOBCENTRIC = True\n",
    "\n",
    "\n",
    "\n",
    "# DATA_NAME = 'social_media_cloud_k_4_L_2_n_4_chancap500_numchans1_mldat2e6_bidirectional'\n",
    "# DATA_NAME = 'rack_dist_sensitivity_0.8_k_4_L_2_n_16_chancap1250_numchans1_mldat3.2e5_bidirectional'\n",
    "DATA_NAME = 'jobcentric_prototyping_k_4_L_2_n_16_chancap1_numchans1_mldat3e3_bidirectional'\n",
    "\n",
    "\n",
    "# path_to_benchmark_data = '/scratch/datasets/trafpy/management/flowcentric/{}_testbed_data_v3.obj'.format(DATA_NAME)\n",
    "# path_to_testbed_data = '/scratch/datasets/trafpy/management/flowcentric/{}_slotsize_1000.0_testbed_data'.format(DATA_NAME)\n",
    "# path_to_testbed_data = '/rdata/ong/trafpy/management/flowcentric/{}_slotsize_1000.0_testbed_data'.format(DATA_NAME)\n",
    "path_to_testbed_data = '/rdata/ong/trafpy/management/jobcentric/{}_slotsize_10.0_testbed_data'.format(DATA_NAME)\n",
    "\n",
    "\n",
    "if os.path.isdir(path_to_testbed_data):\n",
    "    # data split into separate files in a directory\n",
    "    separate_files = True\n",
    "    # load paths to testbed data dirs\n",
    "    testbed_data_list = sorted(glob.glob(path_to_testbed_data + '/*'))\n",
    "    print(testbed_data_list)\n",
    "    envs = []\n",
    "    for testbed_path in testbed_data_list:\n",
    "        try:\n",
    "            filehandler = open(testbed_path+'/simulation.obj', 'rb')\n",
    "            envs.append(pickle.load(filehandler))\n",
    "        except FileNotFoundError:\n",
    "            print('No simulation.obj in {}'.format(testbed_path))\n",
    "        \n",
    "else:\n",
    "    # all data stored in single file\n",
    "    separate_files = False\n",
    "    # load raw testbed data\n",
    "    filehandler = open(path_to_benchmark_data, 'rb')\n",
    "    testbed_data = pickle.load(filehandler)\n",
    "    print(testbed_data.keys())\n",
    "    # unpack env dicts into list of env objects\n",
    "    envs = testbed_data['envs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Overwriting /rdata/ong/trafpy/management/jobcentric/jobcentric_prototyping_k_4_L_2_n_16_chancap1_numchans1_mldat3e3_bidirectional_slotsize_10.0_testbed_data/benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT/env_analyser_database...\n",
      "Computing job arrival metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed job arrival metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 2.6394636631011963 s.\n",
      "Computing job completion metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed job completion metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 0.6051976680755615 s.\n",
      "Computing job queued metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed job queued metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 0.7623918056488037 s.\n",
      "Computing job dropped metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed job dropped metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 0.4835071563720703 s.\n",
      "Computing flow arrival metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed flow arrival metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 1.1333534717559814 s.\n",
      "Computing flow completion metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed flow completion metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 1.0496985912322998 s.\n",
      "Computing flow queued metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed flow queued metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 0.7209687232971191 s.\n",
      "Computing flow dropped metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Computed flow dropped metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 0.687150239944458 s.\n",
      "Saving analyser object for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT...\n",
      "Saved analyser object to /rdata/ong/trafpy/management/jobcentric/jobcentric_prototyping_k_4_L_2_n_16_chancap1_numchans1_mldat3e3_bidirectional_slotsize_10.0_testbed_data/benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT/env_analyser_database in 0.05989241600036621 s.\n",
      "Computed metrics for env benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT in 9.256950378417969 s.\n",
      "\n",
      "-=-=-=-=-=-=--= Summary -=-=-=-=-=-=-=-\n",
      "\n",
      " ~* General Information *~\n",
      "Simulation name: 'benchmark_jobcentric_load_0.1_repeat_0_scheduler_SRPT'\n",
      "Measurement duration: 289.14299242424244 μs (Start time : 30.85700757575759 μs | End time: 320.0 μs)\n",
      "Total number of generated demands (jobs or flows) passed to env: 800\n",
      "Total info arrived: 6950.0 B\n",
      "Total info transported: 6610.0 B\n",
      "Load (abs): 24.036550018831775 B/μs\n",
      "Load (frac): 0.0751142188088493 fraction of network capacity requested.\n",
      "Throughput (abs): 22.86066124093209 B/μs\n",
      "Throughput (frac): 0.9510791366906475 fraction of arrived info successfully transported.\n",
      "T-Score: -9861.28156713443\n",
      "\n",
      " ~* Flow Information *~\n",
      "Total number of generated flows passed to env (src != dst, dependency_type == 'data_dep'): 13720\n",
      "Total number of these flows which arrived during measurement period: 695\n",
      "Time first flow arrived: 37.8945707070707 μs\n",
      "Time last flow arrived: 310.0 μs\n",
      "Total number of flows that were completed: 661\n",
      "Total number of flows that were left in queue at end of measurement period: 34\n",
      "Total number of flows that were dropped (dropped + left in queue at end of measurement period): 34\n",
      "Fraction of arrived flows dropped: 0.04892086330935252\n",
      "Mean flow completion time (FCT): 10.748473769464683 μs\n",
      "99th percentile FCT: 19.248737373737328 μs\n",
      "\n",
      " ~* Job Information *~\n",
      "Total number of generated jobs passed to env: 800\n",
      "Total number of these jobs which arrived during measurement period: 52\n",
      "Time first job arrived: 32.4810606060606 μs\n",
      "Time last job arrived: 308.57007575757586 μs\n",
      "Total number of jobs that were completed: 15\n",
      "Total number of jobs that were left in queue at end of measurement period: 37\n",
      "Total number of jobs that were dropped (dropped + left in queue at end of measurement period): 37\n",
      "Fraction of arrived jobs dropped: 0.7115384615384616\n",
      "Mean job completion time (JCT): 177.5492424242424 μs\n",
      "99th percentile JCT: 243.84526515151515 μs\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import trafpy\n",
    "from trafpy.manager import EnvAnalyser, EnvsPlotter\n",
    "\n",
    "import os\n",
    "\n",
    "# analyse\n",
    "analysers = [EnvAnalyser(env, time_units='\\u03BCs', info_units='B', subject_class_name=env.scheduler.scheduler_name) for env in envs]\n",
    "for analyser in analysers:\n",
    "    env_analyser_database_path = path_to_testbed_data+'/'+analyser.env.sim_name\n",
    "    analyser.compute_metrics(measurement_start_time='auto', # 'auto' None\n",
    "                             measurement_end_time='auto', # 'auto' None\n",
    "                             env_analyser_database_path=env_analyser_database_path,\n",
    "                             overwrite=False,\n",
    "                             print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import trafpy\n",
    "from trafpy.manager import EnvAnalyser, EnvsPlotter\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "# plot\n",
    "plotter = EnvsPlotter(time_units='\\u03BCs', info_units='B')\n",
    "_ = plotter.display_t_score_table(*analysers, display_table=True)\n",
    "# raise Exception()\n",
    "# _ = plotter.plot_t_score_scatter(*analysers, gridlines=True, figsize=(4, 3), legend_ncol=2)\n",
    "\n",
    "# COMMENT IF HAVEN'T TRACKED QUEUE EVOLUTION AND SCHEDULER DECISION COLOUR GRID\n",
    "# _ = plotter.plot_src_dst_queue_evolution_for_different_loads('server_2', 'server_10', 'queue_lengths_num_flows', *analysers)\n",
    "# _ = plotter.plot_src_dst_queue_evolution_for_different_loads('server_2', 'server_10', 'queue_lengths_info_units', *analysers)\n",
    "# _ = plotter.plot_demand_slot_colour_grid_for_different_schedulers(*analysers)\n",
    "\n",
    "# COMMENT IF WANT TO SAVE TIME\n",
    "# _ = plotter.plot_link_utilisation_vs_time_for_different_loads(*analysers, mean_period=10, figsize=(6, 2), legend_ncol=2, plot_legend=False) # mean_period=2500\n",
    "# _ = plotter.plot_link_concurrent_demands_vs_time_for_different_loads(*analysers, mean_period=500)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_ = plotter.plot_demand_completion_time_vs_size_for_different_loads(*analysers, gridlines=True, figsize=(4, 3), legend_ncol=2)\n",
    "_ = plotter.plot_throughput_rate_vs_load(*analysers, plot_bar_charts=False, gridlines=True, figsize=(6, 2), legend_ncol=2)\n",
    "_ = plotter.plot_throughput_frac_vs_load(*analysers, plot_bar_charts=False, gridlines=True, figsize=(6, 2), legend_ncol=2)\n",
    "_ = plotter.plot_fcts_cdf_for_different_loads(*analysers, gridlines=True, figsize=(6, 2), legend_ncol=2)\n",
    "_ = plotter.plot_mean_fct_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "_ = plotter.plot_99th_percentile_fct_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "_ = plotter.plot_max_fct_vs_load(*analysers, gridlines=True, logscale=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "_ = plotter.plot_fraction_of_arrived_flows_dropped_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "_ = plotter.plot_fraction_of_arrived_info_dropped_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "\n",
    "if JOBCENTRIC:\n",
    "    _ = plotter.plot_jcts_cdf_for_different_loads(*analysers, gridlines=True, figsize=(6, 2), legend_ncol=2)\n",
    "    _ = plotter.plot_mean_jct_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "    _ = plotter.plot_99th_percentile_jct_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "    _ = plotter.plot_max_jct_vs_load(*analysers, gridlines=True, logscale=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "    _ = plotter.plot_fraction_of_arrived_jobs_dropped_vs_load(*analysers, logscale=True, gridlines=True, cdf_figsize=(6, 2), scatter_figsize=(4,3), legend_ncol=2)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Time to plot: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM RADAR PLOT(S)\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import trafpy.generator as tpg\n",
    "\n",
    "time_units = '\\u03BCs'\n",
    "info_units = 'B'\n",
    "\n",
    "plot_dict = {'Mean FCT ({})'.format(time_units): {'range': [0, 1], 'classes': {'class1': 0.2,\n",
    "                                                         'class2': 0.4}},\n",
    "             'p99 FCT ({})'.format(time_units): {'range': [0, 100], 'classes': {'class1': 60,\n",
    "                                                           'class2': 20}},\n",
    "             'Max FCT ({})'.format(time_units): {'range': [1, 5], 'classes': {'class1': 2,\n",
    "                                                         'class2': 5}},\n",
    "             'Throughput Rate ({}/{})'.format(info_units, time_units): {'range': [0, 10], 'classes': {'class1': 7,\n",
    "                                                          'class2': 3}},\n",
    "             'Frac Info Dropped': {'range': [0, 10], 'classes': {'class1': 7,\n",
    "                                                          'class2': 3}},\n",
    "             'Frac Flows Dropped': {'range': [10, 0], 'classes': {'class1': 1,\n",
    "                                                          'class2': 2}}\n",
    "            }\n",
    "\n",
    "_ = tpg.plot_radar(plot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOT SCHEDULER SENSITIVITY TO LOAD\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import trafpy.generator as tpg\n",
    "\n",
    "from collections import defaultdict\n",
    "import sigfig\n",
    "\n",
    "\n",
    "def get_summary_dict(analysers, headers, time_units='', info_units=''):\n",
    "    summary_dict = {header: [] for header in headers}\n",
    "    for analyser in analysers:\n",
    "        # self._check_analyser_valid(analyser)\n",
    "        summary_dict['Load'].append(round(analyser.load_frac, 2))\n",
    "        summary_dict['Subject'].append(analyser.subject_class_name)\n",
    "        summary_dict['T-Score'].append(analyser.t_score)\n",
    "        summary_dict['Mean FCT ({})'.format(time_units)].append(round(analyser.mean_fct, 1))\n",
    "        summary_dict['p99 FCT ({})'.format(time_units)].append(round(analyser.nn_fct, 1))\n",
    "        summary_dict['Max FCT ({})'.format(time_units)].append(round(analyser.max_fct, 1))\n",
    "        summary_dict['Throughput Frac'].append(round(analyser.throughput_frac, 1))\n",
    "        summary_dict['Frac Flows Dropped'].append(sigfig.round(analyser.dropped_flow_frac, sigfigs=3))\n",
    "        summary_dict['Frac Info Dropped'].append(sigfig.round(analyser.dropped_info_frac, sigfigs=3))\n",
    "    return summary_dict\n",
    "\n",
    "\n",
    "\n",
    "time_units = '\\u03BCs'\n",
    "info_units = 'B'\n",
    "headers = ['Load',\n",
    "           'T-Score',\n",
    "           'Subject',\n",
    "           'Mean FCT ({})'.format(time_units),\n",
    "           'p99 FCT ({})'.format(time_units),\n",
    "           'Max FCT ({})'.format(time_units),\n",
    "           'Throughput Frac',\n",
    "           'Frac Info Dropped',\n",
    "           'Frac Flows Dropped']\n",
    "dont_plot_headers = ['Load', 'T-Score', 'Subject']\n",
    "plot_headers = [header for header in headers if header not in dont_plot_headers]\n",
    "# determine if higher is better for each header\n",
    "is_higher_better = {}\n",
    "for header in headers:\n",
    "    if header == 'T-Score' or header == 'Throughput Frac':\n",
    "        is_higher_better[header] = True\n",
    "    else:\n",
    "        is_higher_better[header] = False\n",
    "\n",
    "scheduler_to_analysers = {}\n",
    "for analyser in analysers:\n",
    "    if analyser.env.scheduler.scheduler_name not in scheduler_to_analysers:\n",
    "        scheduler_to_analysers[analyser.env.scheduler.scheduler_name] = [analyser]\n",
    "    else:\n",
    "        scheduler_to_analysers[analyser.env.scheduler.scheduler_name].append(analyser)\n",
    "\n",
    "for scheduler_name in scheduler_to_analysers.keys():\n",
    "    nested_dict = lambda: defaultdict(nested_dict)\n",
    "    plot_dict = nested_dict()\n",
    "    \n",
    "    anys = scheduler_to_analysers[scheduler_name]\n",
    "    summary_dict = get_summary_dict(anys, headers, time_units=time_units, info_units=info_units)\n",
    "    \n",
    "    for header in plot_headers:\n",
    "        min_val, max_val = min(summary_dict[header]), max(summary_dict[header])\n",
    "        \n",
    "#         if is_higher_better[header]:\n",
    "#             # want higher (better) values on outer radar edge -> don't flip range\n",
    "#             _range = [0.9*min(summary_dict[header]), 1.1*max(summary_dict[header])]\n",
    "#         else:\n",
    "#             # want lower (better) values on outer radar edge -> flip range\n",
    "#             _range = [1.1*max(summary_dict[header]), 0.9*min(summary_dict[header])]\n",
    "            \n",
    "        diff = max(max_val - min_val, 1e-9)\n",
    "        min_val -= (0.1*diff)\n",
    "        max_val += (0.1*diff)\n",
    "        _range = [min_val, max_val]\n",
    "#         _range = [max(min_val, 1e-9), max(max_val, 1e-9)]\n",
    "        if not is_higher_better[header]:\n",
    "            # flip axis\n",
    "            _range = _range[::-1]\n",
    "\n",
    "        plot_dict[header]['range'] = _range\n",
    "        for i, load in enumerate(summary_dict['Load']):\n",
    "            plot_dict[header]['classes']['Load {}'.format(str(load))] = summary_dict[header][i]\n",
    "        \n",
    "    tpg.plot_radar(plot_dict, \n",
    "                   title=scheduler_name,\n",
    "                   fill=True,\n",
    "                   fill_alpha=0.05,\n",
    "                   figsize=(12.8, 9.6))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "# import trafpy\n",
    "# import trafpy.generator as tpg\n",
    "# from trafpy.manager import RandomAgent, RWA, Demand, DCN, EnvAnalyser\n",
    "# import json\n",
    "\n",
    "# DATA_BAME = 'artificial_light_chancap10'\n",
    "# path_to_benchmark_data = os.path.dirname(trafpy.__file__)+'/../data/benchmark_data/{}_benchmark_data.json'.format(DATA_NAME)\n",
    "# benchmark_data = json.loads(tpg.load_data_from_json(path_to_benchmark_data))\n",
    "# benchmarks = list(benchmark_data.keys())\n",
    "\n",
    "# SLOT_SIZE = 1.0\n",
    "# PACKET_SIZE = 1\n",
    "# NUM_CHANNELS = 1\n",
    "# NUM_K_PATHS = 1\n",
    "# MAX_FLOWS = 10\n",
    "# MAX_TIME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = tpg.gen_fat_tree(k=3, N=2, num_channels=1, server_to_rack_channel_capacity=10)\n",
    "# rwa = RWA(tpg.gen_channel_names(NUM_CHANNELS), NUM_K_PATHS)\n",
    "# scheduler = RandomAgent(network, rwa, slot_size=SLOT_SIZE, packet_size=PACKET_SIZE)\n",
    "\n",
    "# num_benchmark_tests = 0\n",
    "# for benchmark in benchmarks:\n",
    "#     for load in benchmark_data[benchmark]:\n",
    "#         for repeat in benchmark_data[benchmark][load]:\n",
    "#             num_benchmark_tests += 1\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     for load in list(benchmark_data[benchmark].keys()):\n",
    "#         for repeat in benchmark_data[benchmark][load]:\n",
    "#             if json.loads(load) == 0.1 and scheduler.scheduler_name == 'random':\n",
    "#                 demand_data = benchmark_data[benchmark][load][repeat]\n",
    "#                 demand = tpg.Demand(demand_data)\n",
    "#                 env = DCN(network, demand, scheduler, num_k_paths=NUM_K_PATHS, slot_size=SLOT_SIZE, max_flows=MAX_FLOWS, max_time=MAX_TIME)\n",
    "#                 print(env.slot_size)\n",
    "                \n",
    "#                 observation = env.reset()\n",
    "#                 scheduler.register_env(env)\n",
    "                \n",
    "#                 while True:\n",
    "#                     action = scheduler.get_action(observation)\n",
    "#                     observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "#                     if done:\n",
    "#                         print('Completed')\n",
    "#                         analyser = EnvAnalyser(env)\n",
    "#                         analyser.compute_metrics(print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packet size = info\n",
    "# slot size = sec\n",
    "# min bandwidth in network = info per sec\n",
    "\n",
    "# assume that 1 packet size of info can be transferred per sub slot\n",
    "# info per sub slot = 1 packet = packet size\n",
    "# sub slot size = (info per sub slot / min bandwidth in network) * slot size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "if a is not None:\n",
    "    print('not none')\n",
    "else:\n",
    "    print('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_time = 1.001\n",
    "print(round(1-(increment_time-1),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_link_capacity = 500\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
