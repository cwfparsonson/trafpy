{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-12 14:41:53,865\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '128.40.41.23',\n",
       " 'raylet_ip_address': '128.40.41.23',\n",
       " 'redis_address': '128.40.41.23:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-12_14-41-52_956587_196182/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-12_14-41-52_956587_196182/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-12_14-41-52_956587_196182',\n",
       " 'metrics_export_port': 62988}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Checkpoint Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zciccwf/ray_results\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# clear saved agent folder\n",
    "CHECKPOINT_ROOT = 'tmp/ppo/cartpole_v0'\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "# clear ray results folder\n",
    "RAY_RESULTS = os.getenv('HOME') + '/ray_results'\n",
    "print(RAY_RESULTS)\n",
    "shutil.rmtree(RAY_RESULTS, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure RL Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_workers', 'num_envs_per_worker', 'rollout_fragment_length', 'batch_mode', 'num_gpus', 'train_batch_size', 'model', 'optimizer', 'gamma', 'horizon', 'soft_horizon', 'no_done_at_end', 'env_config', 'env', 'normalize_actions', 'clip_rewards', 'clip_actions', 'preprocessor_pref', 'lr', 'monitor', 'log_level', 'callbacks', 'ignore_worker_failures', 'log_sys_usage', 'fake_sampler', 'framework', 'eager_tracing', 'no_eager_on_workers', 'explore', 'exploration_config', 'evaluation_interval', 'evaluation_num_episodes', 'in_evaluation', 'evaluation_config', 'evaluation_num_workers', 'custom_eval_function', 'sample_async', '_use_trajectory_view_api', 'observation_filter', 'synchronize_filters', 'tf_session_args', 'local_tf_session_args', 'compress_observations', 'collect_metrics_timeout', 'metrics_smoothing_episodes', 'remote_worker_envs', 'remote_env_batch_wait_ms', 'min_iter_time_s', 'timesteps_per_iteration', 'seed', 'extra_python_environs_for_driver', 'extra_python_environs_for_worker', 'num_cpus_per_worker', 'num_gpus_per_worker', 'custom_resources_per_worker', 'num_cpus_for_driver', 'memory', 'object_store_memory', 'memory_per_worker', 'object_store_memory_per_worker', 'input', 'input_evaluation', 'postprocess_inputs', 'shuffle_buffer_size', 'output', 'output_compress_columns', 'output_max_file_size', 'multiagent', 'logger_config', 'replay_sequence_length', 'use_critic', 'use_gae', 'lambda', 'kl_coeff', 'sgd_minibatch_size', 'shuffle_sequences', 'num_sgd_iter', 'lr_schedule', 'vf_share_layers', 'vf_loss_coeff', 'entropy_coeff', 'entropy_coeff_schedule', 'clip_param', 'vf_clip_param', 'grad_clip', 'kl_target', 'simple_optimizer', '_fake_gpus'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-12 14:42:03,472\tWARNING util.py:39 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=94578)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy() # use 'proximal policy optimisation' policy optimiser\n",
    "print(config.keys())\n",
    "config['num_gpus'] = 1\n",
    "config['num_workers'] = 1\n",
    "config['eager_tracing'] = False\n",
    "config['log_level'] = 'WARN'\n",
    "\n",
    "agent = ppo.PPOTrainer(config=config, env='CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:872: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:872: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=196365)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward   9.00/ 21.03/ 67.00 len  21.03 saved tmp/ppo/cartpole_v0/checkpoint_1/checkpoint-1\n",
      "  2 reward   9.00/ 43.81/200.00 len  43.81 saved tmp/ppo/cartpole_v0/checkpoint_2/checkpoint-2\n",
      "  3 reward  10.00/ 72.86/200.00 len  72.86 saved tmp/ppo/cartpole_v0/checkpoint_3/checkpoint-3\n",
      "  4 reward  13.00/101.47/200.00 len 101.47 saved tmp/ppo/cartpole_v0/checkpoint_4/checkpoint-4\n",
      "  5 reward  13.00/129.61/200.00 len 129.61 saved tmp/ppo/cartpole_v0/checkpoint_5/checkpoint-5\n",
      "  6 reward  13.00/154.00/200.00 len 154.00 saved tmp/ppo/cartpole_v0/checkpoint_6/checkpoint-6\n",
      "  7 reward  20.00/173.38/200.00 len 173.38 saved tmp/ppo/cartpole_v0/checkpoint_7/checkpoint-7\n",
      "  8 reward  20.00/185.62/200.00 len 185.62 saved tmp/ppo/cartpole_v0/checkpoint_8/checkpoint-8\n",
      "  9 reward  70.00/198.02/200.00 len 198.02 saved tmp/ppo/cartpole_v0/checkpoint_9/checkpoint-9\n",
      " 10 reward  70.00/198.70/200.00 len 198.70 saved tmp/ppo/cartpole_v0/checkpoint_10/checkpoint-10\n",
      " 11 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_11/checkpoint-11\n",
      " 12 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_12/checkpoint-12\n",
      " 13 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_13/checkpoint-13\n",
      " 14 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_14/checkpoint-14\n",
      " 15 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_15/checkpoint-15\n",
      " 16 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_16/checkpoint-16\n",
      " 17 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_17/checkpoint-17\n",
      " 18 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_18/checkpoint-18\n",
      " 19 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_19/checkpoint-19\n",
      " 20 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_20/checkpoint-20\n",
      " 21 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_21/checkpoint-21\n",
      " 22 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_22/checkpoint-22\n",
      " 23 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_23/checkpoint-23\n",
      " 24 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_24/checkpoint-24\n",
      " 25 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_25/checkpoint-25\n",
      " 26 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_26/checkpoint-26\n",
      " 27 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_27/checkpoint-27\n",
      " 28 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_28/checkpoint-28\n",
      " 29 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_29/checkpoint-29\n",
      " 30 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_30/checkpoint-30\n",
      " 31 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_31/checkpoint-31\n",
      " 32 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_32/checkpoint-32\n",
      " 33 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_33/checkpoint-33\n",
      " 34 reward 187.00/199.87/200.00 len 199.87 saved tmp/ppo/cartpole_v0/checkpoint_34/checkpoint-34\n",
      " 35 reward 187.00/199.87/200.00 len 199.87 saved tmp/ppo/cartpole_v0/checkpoint_35/checkpoint-35\n",
      " 36 reward 144.00/199.31/200.00 len 199.31 saved tmp/ppo/cartpole_v0/checkpoint_36/checkpoint-36\n",
      " 37 reward 144.00/199.31/200.00 len 199.31 saved tmp/ppo/cartpole_v0/checkpoint_37/checkpoint-37\n",
      " 38 reward 144.00/199.26/200.00 len 199.26 saved tmp/ppo/cartpole_v0/checkpoint_38/checkpoint-38\n",
      " 39 reward 144.00/199.39/200.00 len 199.39 saved tmp/ppo/cartpole_v0/checkpoint_39/checkpoint-39\n",
      " 40 reward 144.00/199.39/200.00 len 199.39 saved tmp/ppo/cartpole_v0/checkpoint_40/checkpoint-40\n",
      " 41 reward 195.00/199.95/200.00 len 199.95 saved tmp/ppo/cartpole_v0/checkpoint_41/checkpoint-41\n",
      " 42 reward 195.00/199.95/200.00 len 199.95 saved tmp/ppo/cartpole_v0/checkpoint_42/checkpoint-42\n",
      " 43 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_43/checkpoint-43\n",
      " 44 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_44/checkpoint-44\n",
      " 45 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_45/checkpoint-45\n",
      " 46 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_46/checkpoint-46\n",
      " 47 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_47/checkpoint-47\n",
      " 48 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_48/checkpoint-48\n",
      " 49 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_49/checkpoint-49\n",
      " 50 reward 200.00/200.00/200.00 len 200.00 saved tmp/ppo/cartpole_v0/checkpoint_50/checkpoint-50\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "N_ITER = 50\n",
    "s = \"{:3d} | reward {:6.2f}/{:6.2f}/{:6.2f} | len {:6.2f} | saved agent to {}\"\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    # perform 1 iter of training the policy with the PPO algorithm\n",
    "    result = agent.train()\n",
    "    file_name = agent.save(CHECKPOINT_ROOT)\n",
    "    \n",
    "    print(s.format(\n",
    "    i + 1,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    file_name\n",
    "   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          1280        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 2)            514         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 134,915\n",
      "Trainable params: 134,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout a Trained Agent from Saved Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zciccwf/.conda/envs/deep_scheduler/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-11-06 11:53:02,219\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-11-06 11:53:03,774\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-11-06 11:53:03,774\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-11-06 11:53:04.793561: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16945512448\n",
      "*** Aborted at 1604663584 (unix time) try \"date -d @1604663584\" if you are using GNU date ***\n",
      "PC: @                0x0 (unknown)\n",
      "*** SIGABRT (@0x82500003ef5) received by PID 16117 (TID 0x7f1c30f00740) from PID 16117; stack trace: ***\n",
      "    @     0x7f1c30ada630 (unknown)\n",
      "    @     0x7f1c30733387 __GI_raise\n",
      "    @     0x7f1c30734a78 __GI_abort\n",
      "    @     0x7f1bddbcc447 tensorflow::internal::LogMessageFatal::~LogMessageFatal()\n",
      "    @     0x7f1bddf5415d stream_executor::port::internal_statusor::Helper::Crash()\n",
      "    @     0x7f1bd119c90e tensorflow::BaseGPUDeviceFactory::EnablePeerAccess()\n",
      "    @     0x7f1bd11a2f41 tensorflow::BaseGPUDeviceFactory::CreateDevices()\n",
      "    @     0x7f1bd11e73bd tensorflow::DeviceFactory::AddDevices()\n",
      "    @     0x7f1bd55776c8 tensorflow::DirectSessionFactory::NewSession()\n",
      "    @     0x7f1bd126e7db tensorflow::NewSession()\n",
      "    @     0x7f1bd4fa6b26 TF_NewSession\n",
      "    @     0x7f1bd45dee02 tensorflow::TF_NewSessionRef()\n",
      "    @     0x7f1bcf8aff28 _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_6moduleEEUlP8TF_GraphPK17TF_SessionOptionsE8_P10TF_SessionJS5_S8_EJNS_4nameENS_5scopeENS_7siblingENS_19return_value_policyEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNEST_\n",
      "    @     0x7f1bcf893e7d pybind11::cpp_function::dispatcher()\n",
      "    @     0x55e4033b4c1e cfunction_call_varargs\n",
      "    @     0x55e4033a9fff _PyObject_MakeTpCall\n",
      "    @     0x55e40345c394 _PyEval_EvalFrameDefault\n",
      "    @     0x55e4034438f0 _PyEval_EvalCodeWithName\n",
      "    @     0x55e403444e74 _PyFunction_Vectorcall\n",
      "    @     0x55e4033e0a5e method_vectorcall\n",
      "    @     0x55e4034585b9 _PyEval_EvalFrameDefault\n",
      "    @     0x55e403444099 _PyEval_EvalCodeWithName\n",
      "    @     0x55e403444e74 _PyFunction_Vectorcall\n",
      "    @     0x55e40342c69a slot_tp_init\n",
      "    @     0x55e4033a9e98 _PyObject_MakeTpCall\n",
      "    @     0x55e40345c3dc _PyEval_EvalFrameDefault\n",
      "    @     0x55e403444099 _PyEval_EvalCodeWithName\n",
      "    @     0x55e403444e74 _PyFunction_Vectorcall\n",
      "    @     0x55e40345773a _PyEval_EvalFrameDefault\n",
      "    @     0x55e403444099 _PyEval_EvalCodeWithName\n",
      "    @     0x55e403444e74 _PyFunction_Vectorcall\n",
      "    @     0x55e40342c69a slot_tp_init\n"
     ]
    }
   ],
   "source": [
    "!rllib rollout tmp/ppo/cartpole_v0/checkpoint_50/checkpoint-50 --config \"{\\\"env\\\": \\\"CartPole-v0\\\"}\" --run PPO --steps 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
