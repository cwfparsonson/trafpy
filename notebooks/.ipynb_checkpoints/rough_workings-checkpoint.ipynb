{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load data from /home/zciccwf/phd_project/projects/trafpy/trafpy/../data/benchmark_data/ndf50_100s_university_benchmark_data.json: 29.834062099456787 s\n"
     ]
    }
   ],
   "source": [
    "import trafpy\n",
    "import trafpy.generator as tpg\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "path_to_benchmark_data = os.path.dirname(trafpy.__file__)+'/../data/benchmark_data/ndf50_1s_university_benchmark_data.json'\n",
    "benchmark_data = json.loads(tpg.load_data_from_json(path_to_benchmark_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets of demand data: 10\n"
     ]
    }
   ],
   "source": [
    "benchmarks = list(benchmark_data.keys())\n",
    "demand_data_list = []\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    for load in benchmark_data[benchmark]:\n",
    "        for repeat in benchmark_data[benchmark][load]:\n",
    "            demand_data = benchmark_data[benchmark][load][repeat]\n",
    "            demand_data_list.append(demand_data)\n",
    "            \n",
    "num_sets_of_demand_data = len(demand_data_list)\n",
    "print('Number of sets of demand data: {}'.format(num_sets_of_demand_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Keys of first demand data set:\\n{}'.format(demand_data_list[0].keys()))\n",
    "\n",
    "print('\\nStructure of first demand data set:\\n{}'.format(demand_data_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num flows in each set of demand data:\n",
      "[192000, 384000, 768000, 768000, 1536000, 1536000, 1536000, 1536000, 3072000, 3072000]\n",
      "Times first flow arrived in each set of demand data:\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Times last flow arrived in each set of demand data:\n",
      "[105390440.45601942, 105440866.29540928, 139388200.00680223, 108035188.34903812, 168747325.04215565, 138813227.9279127, 118314302.09514146, 103462284.5038257, 193399897.09594557, 169479132.78175196]\n",
      "Total info arrived for each set:\n",
      "[621018240.0, 1255016192.0, 2486637312.0, 2549064448.0, 4962469376.0, 4960633344.0, 4924089856.0, 4937352192.0, 10038974464.0, 10130619392.0]\n",
      "Load rates for each set:\n",
      "[5.8925481031570195, 11.902559568164714, 17.839654374463912, 23.594760993654482, 29.40769209088381, 35.73602759656388, 41.61872038124634, 47.72127559021213, 51.907858353304455, 59.77502495865247]\n",
      "Smallest flow size for each set:\n",
      "[5.0, 5.0, 5.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0]\n",
      "Largest flow size for each set:\n",
      "[11379.0, 11379.0, 11380.0, 11378.0, 11371.0, 11379.0, 11379.0, 11377.0, 11376.0, 11377.0]\n",
      "Num slots in each set of demand data:\n",
      "[105390, 105440, 139388, 108035, 168747, 138813, 118314, 103462, 193399, 169479]\n"
     ]
    }
   ],
   "source": [
    "num_flows_list = []\n",
    "time_first_flow_arrived_list = []\n",
    "time_last_flow_arrived_list = []\n",
    "smallest_flow_size_list = []\n",
    "largest_flow_size_list = []\n",
    "total_info_arrived = []\n",
    "load_rate = []\n",
    "for demand_data in demand_data_list:\n",
    "    num_flows_list.append(len(demand_data['flow_id']))\n",
    "    time_first_flow_arrived_list.append(min(demand_data['event_time']))\n",
    "    time_last_flow_arrived_list.append(max(demand_data['event_time']))\n",
    "    total_info_arrived.append(sum(demand_data['flow_size']))\n",
    "    load_rate.append(sum(demand_data['flow_size'])/(max(demand_data['event_time'])-min(demand_data['event_time'])))\n",
    "    smallest_flow_size_list.append(min(demand_data['flow_size']))\n",
    "    largest_flow_size_list.append(max(demand_data['flow_size']))\n",
    "\n",
    "print('Num flows in each set of demand data:\\n{}'.format(num_flows_list))\n",
    "print('Times first flow arrived in each set of demand data:\\n{}'.format(time_first_flow_arrived_list))\n",
    "print('Times last flow arrived in each set of demand data:\\n{}'.format(time_last_flow_arrived_list))\n",
    "print('Total info arrived for each set:\\n{}'.format(total_info_arrived))\n",
    "print('Load rates for each set:\\n{}'.format(load_rate))\n",
    "print('Smallest flow size for each set:\\n{}'.format(smallest_flow_size_list))\n",
    "print('Largest flow size for each set:\\n{}'.format(largest_flow_size_list))\n",
    "\n",
    "slot_size = 1e3\n",
    "num_slots_list = [int(_time/slot_size) for _time in time_last_flow_arrived_list]\n",
    "print('Num slots in each set of demand data:\\n{}'.format(num_slots_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0'])\n",
      "benchmark: university | load: 0.1 | repeat: 0\n",
      "benchmark: university | load: 0.2 | repeat: 0\n",
      "benchmark: university | load: 0.3 | repeat: 0\n",
      "benchmark: university | load: 0.4 | repeat: 0\n",
      "benchmark: university | load: 0.5 | repeat: 0\n",
      "benchmark: university | load: 0.6 | repeat: 0\n",
      "benchmark: university | load: 0.7 | repeat: 0\n",
      "benchmark: university | load: 0.8 | repeat: 0\n",
      "benchmark: university | load: 0.9 | repeat: 0\n",
      "benchmark: university | load: 1.0 | repeat: 0\n"
     ]
    }
   ],
   "source": [
    "for benchmark in benchmarks:\n",
    "    print(benchmark_data[benchmark].keys())\n",
    "    for load in list(benchmark_data[benchmark].keys()):\n",
    "        for repeat in benchmark_data[benchmark][load]:\n",
    "            print('benchmark: {} | load: {} | repeat: {}'.format(benchmark,load,repeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last event time: 169479132.78175196\n",
      "dict_keys(['flow_id', 'sn', 'dn', 'flow_size', 'event_time', 'establish', 'index'])\n"
     ]
    }
   ],
   "source": [
    "dd = demand_data_list[-1]\n",
    "print('Last event time: {}'.format(max(demand_data['event_time'])))\n",
    "print(dd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
