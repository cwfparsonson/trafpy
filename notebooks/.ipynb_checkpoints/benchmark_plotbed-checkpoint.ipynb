{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRPT vs. BASRPT vs. Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import trafpy\n",
    "import trafpy.generator as tpg\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_NAME = 'university_chancap500_numchans1_mldat2e6_bidirectional'\n",
    "# DATA_NAME = 'ndf50_mldat6e6_load0.4_university'\n",
    "path_to_benchmark_data = os.path.dirname(trafpy.__file__)+'/../data/testbed_data/{}_testbed_data_v4.obj'.format(DATA_NAME)\n",
    "filehandler = open(path_to_benchmark_data, 'rb')\n",
    "tb_dict = pickle.load(filehandler)\n",
    "\n",
    "print(tb_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# unpack env dicts into list of env objects\n",
    "envs = tb_dict['envs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import trafpy\n",
    "from trafpy.manager import EnvAnalyser, EnvsPlotter\n",
    "\n",
    "# analyse\n",
    "analysers = [EnvAnalyser(env, env.scheduler.scheduler_name) for env in envs]\n",
    "for analyser in analysers:\n",
    "    analyser.compute_metrics(measurement_start_time='auto',\n",
    "                             measurement_end_time='auto',\n",
    "                             print_summary=True)\n",
    "# plot\n",
    "plotter = EnvsPlotter()\n",
    "# _ = plotter.plot_src_dst_queue_evolution_for_different_loads('server_2', 'server_10', 'queue_lengths_num_flows', *analysers)\n",
    "# _ = plotter.plot_src_dst_queue_evolution_for_different_loads('server_2', 'server_10', 'queue_lengths_info_units', *analysers)\n",
    "# _ = plotter.plot_demand_slot_colour_grid_for_different_schedulers(*analysers)\n",
    "_ = plotter.plot_link_utilisation_vs_time_for_different_loads(*analysers, mean_period=500)\n",
    "_ = plotter.plot_link_concurrent_demands_vs_time_for_different_loads(*analysers, mean_period=500)\n",
    "_ = plotter.plot_demand_completion_time_vs_size_for_different_loads(*analysers)\n",
    "_ = plotter.plot_throughput_vs_load(*analysers)\n",
    "_ = plotter.plot_average_fct_vs_load(*analysers)\n",
    "_ = plotter.plot_99th_percentile_fct_vs_load(*analysers)\n",
    "_ = plotter.plot_max_fct_vs_load(*analysers)\n",
    "_ = plotter.plot_fraction_of_arrived_flows_dropped_vs_load(*analysers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "# import trafpy\n",
    "# import trafpy.generator as tpg\n",
    "# from trafpy.manager import RandomAgent, RWA, Demand, DCN, EnvAnalyser\n",
    "# import json\n",
    "\n",
    "# DATA_BAME = 'artificial_light_chancap10'\n",
    "# path_to_benchmark_data = os.path.dirname(trafpy.__file__)+'/../data/benchmark_data/{}_benchmark_data.json'.format(DATA_NAME)\n",
    "# benchmark_data = json.loads(tpg.load_data_from_json(path_to_benchmark_data))\n",
    "# benchmarks = list(benchmark_data.keys())\n",
    "\n",
    "# SLOT_SIZE = 1.0\n",
    "# PACKET_SIZE = 1\n",
    "# NUM_CHANNELS = 1\n",
    "# NUM_K_PATHS = 1\n",
    "# MAX_FLOWS = 10\n",
    "# MAX_TIME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = tpg.gen_fat_tree(k=3, N=2, num_channels=1, server_to_rack_channel_capacity=10)\n",
    "# rwa = RWA(tpg.gen_channel_names(NUM_CHANNELS), NUM_K_PATHS)\n",
    "# scheduler = RandomAgent(network, rwa, slot_size=SLOT_SIZE, packet_size=PACKET_SIZE)\n",
    "\n",
    "# num_benchmark_tests = 0\n",
    "# for benchmark in benchmarks:\n",
    "#     for load in benchmark_data[benchmark]:\n",
    "#         for repeat in benchmark_data[benchmark][load]:\n",
    "#             num_benchmark_tests += 1\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     for load in list(benchmark_data[benchmark].keys()):\n",
    "#         for repeat in benchmark_data[benchmark][load]:\n",
    "#             if json.loads(load) == 0.1 and scheduler.scheduler_name == 'random':\n",
    "#                 demand_data = benchmark_data[benchmark][load][repeat]\n",
    "#                 demand = tpg.Demand(demand_data)\n",
    "#                 env = DCN(network, demand, scheduler, num_k_paths=NUM_K_PATHS, slot_size=SLOT_SIZE, max_flows=MAX_FLOWS, max_time=MAX_TIME)\n",
    "#                 print(env.slot_size)\n",
    "                \n",
    "#                 observation = env.reset()\n",
    "#                 scheduler.register_env(env)\n",
    "                \n",
    "#                 while True:\n",
    "#                     action = scheduler.get_action(observation)\n",
    "#                     observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "#                     if done:\n",
    "#                         print('Completed')\n",
    "#                         analyser = EnvAnalyser(env)\n",
    "#                         analyser.compute_metrics(print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge ('core_0', 'agg_0'): 40000\n",
      "Edge ('core_0', 'agg_1'): 40000\n",
      "Edge ('core_0', 'agg_2'): 40000\n",
      "Edge ('core_1', 'agg_0'): 40000\n",
      "Edge ('core_1', 'agg_1'): 40000\n",
      "Edge ('core_1', 'agg_2'): 40000\n",
      "Edge ('rack_0', 'edge_0'): 10000\n",
      "Edge ('rack_0', 'server_0'): 500\n",
      "Edge ('rack_0', 'server_1'): 500\n",
      "Edge ('rack_1', 'edge_0'): 10000\n",
      "Edge ('rack_1', 'server_2'): 500\n",
      "Edge ('rack_1', 'server_3'): 500\n",
      "Edge ('rack_2', 'edge_1'): 10000\n",
      "Edge ('rack_2', 'server_4'): 500\n",
      "Edge ('rack_2', 'server_5'): 500\n",
      "Edge ('rack_3', 'edge_1'): 10000\n",
      "Edge ('rack_3', 'server_6'): 500\n",
      "Edge ('rack_3', 'server_7'): 500\n",
      "Edge ('rack_4', 'edge_2'): 10000\n",
      "Edge ('rack_4', 'server_8'): 500\n",
      "Edge ('rack_4', 'server_9'): 500\n",
      "Edge ('rack_5', 'edge_2'): 10000\n",
      "Edge ('rack_5', 'server_10'): 500\n",
      "Edge ('rack_5', 'server_11'): 500\n",
      "Edge ('edge_0', 'agg_0'): 40000\n",
      "Edge ('edge_1', 'agg_1'): 40000\n",
      "Edge ('edge_2', 'agg_2'): 40000\n"
     ]
    }
   ],
   "source": [
    "for analyser in analysers:\n",
    "    net = analyser.env.network\n",
    "    for edge in net.edges:\n",
    "        print('Edge {}: {}'.format(edge, net[edge[0]][edge[1]]['max_channel_capacity']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
