{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand config file imported.\n"
     ]
    }
   ],
   "source": [
    "import trafpy.generator as tpg\n",
    "import config\n",
    "\n",
    "endpoints = ['server_'+str(i) for i in range(5)]\n",
    "node_dist,fig = tpg.gen_multimodal_node_dist(eps=endpoints,skewed_nodes=['server_1','server_2'],show_fig=True)\n",
    "\n",
    "prob_dist, rand_vars, fig = tpg.gen_multimodal_val_dist(min_val=10,max_val=7000,locations=[20,4000],skews=[6,-1],scales=[150,1500],num_skew_samples=[10000,650],bg_factor=0.05,return_data=True,show_fig=True,logscale=True,xlim=[10,10000],occurrence_multiplier=10,num_bins=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_uniform_multinomial_exp_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_multimodal_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_multimodal_node_pair_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tpg.gen_uniform_val_dist(min_val=0,\n",
    "                             max_val=100,\n",
    "                             round_to_nearest=1,\n",
    "                             show_fig=True,\n",
    "                             print_data=False,\n",
    "                             num_bins=101)\n",
    "\n",
    "_ = tpg.gen_multimodal_val_dist(min_val=10,\n",
    "                               max_val=7000,\n",
    "                               locations=[20,4000],\n",
    "                               skews=[6,-1],\n",
    "                               scales=[150,1500],\n",
    "                               num_skew_samples=[10000,650],\n",
    "                               bg_factor=0.05,\n",
    "                               show_fig=True,\n",
    "                               print_data=False,\n",
    "                               logscale=True,\n",
    "                               xlim=[10,10000],\n",
    "                               num_bins=18)\n",
    "\n",
    "_ = tpg.gen_named_val_dist(dist='weibull',\n",
    "                          params={'_alpha': 1.4, '_lambda': 7000},\n",
    "                          show_fig=True,\n",
    "                          print_data=False,\n",
    "                          logscale=True,\n",
    "                          xlim=[1e2,1e11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Job-Centric Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "node_dist = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=False)\n",
    "\n",
    "flow_size_dist = tpg.gen_multimodal_val_dist(config.MIN_FLOW_SIZE,\n",
    "                                            config.MAX_FLOW_SIZE,\n",
    "                                            locations=[50],\n",
    "                                            skews=[0],\n",
    "                                            scales=[10],\n",
    "                                            num_skew_samples=[10000],\n",
    "                                            bg_factor=0,\n",
    "                                            round_to_nearest=1,\n",
    "                                            show_fig=True,\n",
    "                                            num_bins=34,\n",
    "                                            print_data=False)\n",
    "\n",
    "interarrival_time_dist = tpg.gen_multimodal_val_dist(config.MIN_INTERARRIVAL,\n",
    "                                                    config.MAX_INTERARRIVAL,\n",
    "                                                    locations=[1, 1, 3000, 1, 1800000, 10000000],\n",
    "                                                    skews=[0, 100, -10, 10, 50, 6],\n",
    "                                                    scales=[0.1, 62, 2000, 7500, 3500000, 20000000],\n",
    "                                                    num_skew_samples=[800, 1000, 2000, 4000, 4000, 3000],\n",
    "                                                    bg_factor=0.025,\n",
    "                                                    round_to_nearest=1,\n",
    "                                                    show_fig=True,\n",
    "                                                    print_data=False)\n",
    "\n",
    "num_ops_dist = tpg.gen_multimodal_val_dist(config.MIN_NUM_OPS,\n",
    "                                          config.MAX_NUM_OPS,\n",
    "                                          locations=[100],\n",
    "                                          skews=[0.05],\n",
    "                                          scales=[50],\n",
    "                                          num_skew_samples=[10000],\n",
    "                                          bg_factor=0.05,\n",
    "                                          round_to_nearest=1,\n",
    "                                          show_fig=True,\n",
    "                                          print_data=False)\n",
    "\n",
    "\n",
    "job_centric_demand_data = tpg.create_demand_data(num_demands=config.NUM_DEMANDS,\n",
    "                                                eps=config.ENDPOINT_LABELS,\n",
    "                                                node_dist=node_dist,\n",
    "                                                flow_size_dist=flow_size_dist,\n",
    "                                                interarrival_time_dist=interarrival_time_dist,\n",
    "                                                num_ops_dist=num_ops_dist,\n",
    "                                                c=config.C,\n",
    "                                                use_multiprocessing=False,\n",
    "                                                print_data=True)\n",
    "print('Job data:\\n{}'.format(job_centric_demand_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Flow-Centric Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dist = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=False)\n",
    "\n",
    "flow_size_dist = tpg.gen_named_val_dist(dist='weibull',\n",
    "                                       params={'_alpha': 1.4, '_lambda': 7000},\n",
    "                                       show_fig=True,\n",
    "                                       print_data=False,\n",
    "                                       logscale=True,\n",
    "                                       round_to_nearest=1,\n",
    "                                       xlim=[1e2,1e12])\n",
    "\n",
    "interarrival_time_dist = tpg.gen_named_val_dist(dist='lognormal',\n",
    "                                               params={'_mu': 7.4, '_sigma': 2},\n",
    "                                               show_fig=True,\n",
    "                                               print_data=False,\n",
    "                                               logscale=True,\n",
    "                                               xlim=[1e1,1e6])\n",
    "\n",
    "\n",
    "flow_centric_demand_data = tpg.create_demand_data(num_demands=config.NUM_DEMANDS,\n",
    "                                                 eps=config.ENDPOINT_LABELS,\n",
    "                                                 node_dist=node_dist,\n",
    "                                                 flow_size_dist=flow_size_dist,\n",
    "                                                 interarrival_time_dist=interarrival_time_dist,\n",
    "                                                 print_data=True)\n",
    "print('Flow data:\\n{}'.format(flow_centric_demand_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise into Slots Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO THIS INTERNALLY IN DEMAND OBJECT WHEN PASS DEMAND INTO ENV\n",
    "\n",
    "# slot_size = 100000\n",
    "\n",
    "# job_centric_slot_dict = tpg.construct_demand_slots_dict(demand_data=job_centric_demand_data,\n",
    "#                                                        slot_size=slot_size)\n",
    "# # print('\\n\\nJob slot dict:\\n{}'.format(job_centric_slot_dict))\n",
    "\n",
    "\n",
    "# flow_centric_slot_dict = tpg.construct_demand_slots_dict(demand_data=flow_centric_demand_data,\n",
    "#                                                         slot_size=slot_size)\n",
    "# # print('\\n\\nFlow slot dict:\\n{}'.format(flow_centric_slot_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpg.pickle_data(data=flow_centric_demand_data,\n",
    "               path_to_save='data/flow_centric_demand_data.pickle',\n",
    "               overwrite=True,\n",
    "               zip_data=True)\n",
    "\n",
    "tpg.pickle_data(path_to_save='data/job_centric_demand_data.pickle',\n",
    "               data=job_centric_demand_data,\n",
    "               overwrite=True,\n",
    "               zip_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_centric_demand_data = tpg.unpickle_data(path_to_load='data/flow_centric_demand_data.pickle',\n",
    "                                             zip_data=True)\n",
    "job_centric_demand_data = tpg.unpickle_data(path_to_load='data/job_centric_demand_data.pickle',\n",
    "                                             zip_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "import config\n",
    "\n",
    "job_centric_demand_data = tpg.unpickle_data(path_to_load='data/job_centric_demand_data.pickle',\n",
    "                                            zip_data=True)\n",
    "\n",
    "\n",
    "dep_stats = tpg.get_job_demand_data_dependency_stats(job_centric_demand_data)\n",
    "\n",
    "# get dependency stats\n",
    "flow_sizes = []\n",
    "for job_id in dep_stats.keys():\n",
    "    for dep in dep_stats[job_id]:\n",
    "        if dep['attr_dict']['dependency_type'] == 'data_dep' and dep['attr_dict']['flow_size'] != 0:\n",
    "            flow_sizes.append(dep['attr_dict']['flow_size'])\n",
    "# get job stats\n",
    "num_ops = []\n",
    "num_edges = []\n",
    "for idx in range(len(job_centric_demand_data['job_id'])):\n",
    "    establish = job_centric_demand_data['establish'][idx]\n",
    "    if establish == 0:\n",
    "        # take down, ignore\n",
    "        pass\n",
    "    else:\n",
    "        # connection\n",
    "        job = job_centric_demand_data['job'][idx]\n",
    "        num_ops.append(job.number_of_nodes())\n",
    "        num_edges.append(job.number_of_edges())\n",
    "# use multiprocessing for graph diameters\n",
    "diameters = tpg.calc_graph_diameters(job_centric_demand_data['job'],multiprocessing_type='pool')\n",
    "\n",
    "# plot\n",
    "_ = tpg.draw_job_graphs(job_centric_demand_data,show_fig=True)\n",
    "_ = tpg.plot_val_dist(flow_sizes, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Flow Sizes',\n",
    "                      num_bins=0)\n",
    "_ = tpg.plot_val_dist(num_ops, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Num Ops',\n",
    "                      num_bins=10)\n",
    "_ = tpg.plot_val_dist(num_edges, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Num Edges',\n",
    "                      num_bins=10)\n",
    "_ = tpg.plot_val_dist(diameters, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Diameters',\n",
    "                      num_bins=10)\n",
    "\n",
    "\n",
    "\n",
    "# problem: last 2 bins/hists being combined into single bar so get peak and get one bar removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Manager Objects & Running Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "from trafpy.manager import Demand, RWA, SRPT, DCN\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load demand data\n",
    "demand_data = tpg.unpickle_data(path_to_load='data/job_centric_demand_data.pickle',\n",
    "                                zip_data=True)\n",
    "\n",
    "# init manager\n",
    "network = tpg.gen_simple_network(ep_label=config.ENDPOINT_LABEL,num_channels=config.NUM_CHANNELS)\n",
    "demand = Demand(demand_data=demand_data)\n",
    "rwa = RWA(tpg.gen_channel_names(config.NUM_CHANNELS), config.NUM_K_PATHS)\n",
    "scheduler = SRPT(network, rwa, slot_size=config.SLOT_SIZE)\n",
    "env = DCN(network, demand, scheduler, slot_size=config.SLOT_SIZE, max_flows=config.MAX_FLOWS, max_time=config.MAX_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulation\n",
    "for episode in range(config.NUM_EPISODES):\n",
    "    print('\\nEpisode {}/{}'.format(episode+1,config.NUM_EPISODES))\n",
    "    observation = env.reset(config.LOAD_DEMANDS)\n",
    "    while True:\n",
    "        print('Time: {}'.format(env.curr_time))\n",
    "        action = scheduler.get_action(observation)\n",
    "        print('Action:\\n{}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Episode finished.')\n",
    "            env.get_scheduling_session_summary(print_summary=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tpg.gen_simple_network(ep_label='server', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
