{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_uniform_multinomial_exp_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_multimodal_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)\n",
    "_ = tpg.gen_multimodal_node_pair_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tpg.gen_uniform_val_dist(min_val=0,\n",
    "                             max_val=100,\n",
    "                             round_to_nearest=1,\n",
    "                             show_fig=True,\n",
    "                             print_data=False,\n",
    "                             num_bins=101)\n",
    "\n",
    "_ = tpg.gen_multimodal_val_dist(min_val=10,\n",
    "                               max_val=7000,\n",
    "                               locations=[20,4000],\n",
    "                               skews=[6,-1],\n",
    "                               scales=[150,1500],\n",
    "                               num_skew_samples=[10000,650],\n",
    "                               bg_factor=0.05,\n",
    "                               show_fig=True,\n",
    "                               print_data=False,\n",
    "                               logscale=True,\n",
    "                               xlim=[10,10000],\n",
    "                               num_bins=18)\n",
    "\n",
    "_ = tpg.gen_named_val_dist(dist='weibull',\n",
    "                          params={'_alpha': 1.4, '_lambda': 7000},\n",
    "                          show_fig=True,\n",
    "                          print_data=False,\n",
    "                          logscale=True,\n",
    "                          xlim=[1e2,1e11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Job-Centric Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_dist = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=False)\n",
    "\n",
    "flow_size_dist, _ = tpg.gen_multimodal_val_dist(config.MIN_FLOW_SIZE,\n",
    "                                            config.MAX_FLOW_SIZE,\n",
    "                                            locations=[50],\n",
    "                                            skews=[0],\n",
    "                                            scales=[10],\n",
    "                                            num_skew_samples=[10000],\n",
    "                                            bg_factor=0,\n",
    "                                            round_to_nearest=1,\n",
    "                                            show_fig=True,\n",
    "                                            num_bins=34,\n",
    "                                            print_data=False)\n",
    "\n",
    "interarrival_time_dist, _ = tpg.gen_multimodal_val_dist(config.MIN_INTERARRIVAL,\n",
    "                                                    config.MAX_INTERARRIVAL,\n",
    "                                                    locations=[1, 1, 3000, 1, 1800000, 10000000],\n",
    "                                                    skews=[0, 100, -10, 10, 50, 6],\n",
    "                                                    scales=[0.1, 62, 2000, 7500, 3500000, 20000000],\n",
    "                                                    num_skew_samples=[800, 1000, 2000, 4000, 4000, 3000],\n",
    "                                                    bg_factor=0.025,\n",
    "                                                    round_to_nearest=1,\n",
    "                                                    show_fig=True,\n",
    "                                                    print_data=False)\n",
    "\n",
    "num_ops_dist, _ = tpg.gen_multimodal_val_dist(config.MIN_NUM_OPS,\n",
    "                                          config.MAX_NUM_OPS,\n",
    "                                          locations=[100],\n",
    "                                          skews=[0.05],\n",
    "                                          scales=[50],\n",
    "                                          num_skew_samples=[10000],\n",
    "                                          bg_factor=0.05,\n",
    "                                          round_to_nearest=1,\n",
    "                                          show_fig=True,\n",
    "                                          print_data=False)\n",
    "\n",
    "\n",
    "job_centric_demand_data, _ = tpg.create_demand_data(num_demands=config.NUM_DEMANDS,\n",
    "                                                eps=config.ENDPOINT_LABELS,\n",
    "                                                node_dist=node_dist,\n",
    "                                                flow_size_dist=flow_size_dist,\n",
    "                                                interarrival_time_dist=interarrival_time_dist,\n",
    "                                                num_ops_dist=num_ops_dist,\n",
    "                                                c=config.C,\n",
    "                                                use_multiprocessing=False,\n",
    "                                                print_data=True)\n",
    "print('Job data:\\n{}'.format(job_centric_demand_data))\n",
    "\n",
    "jobs = list(job_centric_demand_data['job'][0:2])\n",
    "fig = tpg.draw_job_graphs(job_graphs=jobs,show_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Flow-Centric Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_dist = tpg.gen_uniform_node_dist(config.ENDPOINT_LABELS, show_fig=True, print_data=False)\n",
    "\n",
    "flow_size_dist, _ = tpg.gen_named_val_dist(dist='weibull',\n",
    "                                       params={'_alpha': 1.4, '_lambda': 7000},\n",
    "                                       show_fig=True,\n",
    "                                       print_data=False,\n",
    "                                       logscale=True,\n",
    "                                       round_to_nearest=1,\n",
    "                                       xlim=[1e2,1e12])\n",
    "\n",
    "interarrival_time_dist, _ = tpg.gen_named_val_dist(dist='lognormal',\n",
    "                                               params={'_mu': 7.4, '_sigma': 2},\n",
    "                                               show_fig=True,\n",
    "                                               print_data=False,\n",
    "                                               logscale=True,\n",
    "                                               xlim=[1e1,1e6])\n",
    "\n",
    "\n",
    "flow_centric_demand_data, _ = tpg.create_demand_data(num_demands=config.NUM_DEMANDS,\n",
    "                                                 eps=config.ENDPOINT_LABELS,\n",
    "                                                 node_dist=node_dist,\n",
    "                                                 flow_size_dist=flow_size_dist,\n",
    "                                                 interarrival_time_dist=interarrival_time_dist,\n",
    "                                                 print_data=True)\n",
    "print('Flow data:\\n{}'.format(flow_centric_demand_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise into Slots Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO THIS INTERNALLY IN DEMAND OBJECT WHEN PASS DEMAND INTO ENV\n",
    "\n",
    "# slot_size = 100000\n",
    "\n",
    "# job_centric_slot_dict = tpg.construct_demand_slots_dict(demand_data=job_centric_demand_data,\n",
    "#                                                        slot_size=slot_size)\n",
    "# # print('\\n\\nJob slot dict:\\n{}'.format(job_centric_slot_dict))\n",
    "\n",
    "\n",
    "# flow_centric_slot_dict = tpg.construct_demand_slots_dict(demand_data=flow_centric_demand_data,\n",
    "#                                                         slot_size=slot_size)\n",
    "# # print('\\n\\nFlow slot dict:\\n{}'.format(flow_centric_slot_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpg.pickle_data(data=flow_centric_demand_data,\n",
    "               path_to_save='data/flow_centric_demand_data.pickle',\n",
    "               overwrite=True,\n",
    "               zip_data=True)\n",
    "\n",
    "tpg.pickle_data(path_to_save='data/job_centric_demand_data.pickle',\n",
    "               data=job_centric_demand_data,\n",
    "               overwrite=True,\n",
    "               zip_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flow_centric_demand_data = tpg.unpickle_data(path_to_load='data/flow_centric_demand_data.pickle',\n",
    "                                             zip_data=True)\n",
    "job_centric_demand_data = tpg.unpickle_data(path_to_load='data/job_centric_demand_data.pickle',\n",
    "                                             zip_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "import config\n",
    "\n",
    "job_centric_demand_data = tpg.unpickle_data(path_to_load='data/job_centric_demand_data.pickle',\n",
    "                                            zip_data=True)\n",
    "\n",
    "\n",
    "dep_stats = tpg.get_job_demand_data_dependency_stats(job_centric_demand_data)\n",
    "\n",
    "# get dependency stats\n",
    "flow_sizes = []\n",
    "for job_id in dep_stats.keys():\n",
    "    for dep in dep_stats[job_id]:\n",
    "        if dep['attr_dict']['dependency_type'] == 'data_dep' and dep['attr_dict']['flow_size'] != 0:\n",
    "            flow_sizes.append(dep['attr_dict']['flow_size'])\n",
    "# get job stats\n",
    "num_ops = []\n",
    "num_edges = []\n",
    "for idx in range(len(job_centric_demand_data['job_id'])):\n",
    "    establish = job_centric_demand_data['establish'][idx]\n",
    "    if establish == 0:\n",
    "        # take down, ignore\n",
    "        pass\n",
    "    else:\n",
    "        # connection\n",
    "        job = job_centric_demand_data['job'][idx]\n",
    "        num_ops.append(job.number_of_nodes())\n",
    "        num_edges.append(job.number_of_edges())\n",
    "# use multiprocessing for graph diameters\n",
    "diameters = tpg.calc_graph_diameters(job_centric_demand_data['job'],multiprocessing_type='pool')\n",
    "\n",
    "# plot\n",
    "_ = tpg.draw_job_graphs(job_centric_demand_data,show_fig=True)\n",
    "_ = tpg.plot_val_dist(flow_sizes, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Flow Sizes',\n",
    "                      num_bins=0)\n",
    "_ = tpg.plot_val_dist(num_ops, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Num Ops',\n",
    "                      num_bins=10)\n",
    "_ = tpg.plot_val_dist(num_edges, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Num Edges',\n",
    "                      num_bins=10)\n",
    "_ = tpg.plot_val_dist(diameters, \n",
    "                      show_fig=True,\n",
    "                      rand_var_name='Diameters',\n",
    "                      num_bins=10)\n",
    "\n",
    "\n",
    "\n",
    "# problem: last 2 bins/hists being combined into single bar so get peak and get one bar removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Manager Objects & Running Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "from trafpy.manager import Demand, RWA, SRPT, DCN\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load demand data\n",
    "# demand_data = tpg.unpickle_data(path_to_load='data/flow_centric_demand_data.pickle',\n",
    "#                                 zip_data=True)\n",
    "demand_data = flow_centric_demand_data\n",
    "\n",
    "# init manager\n",
    "network = tpg.gen_simple_network(ep_label=config.ENDPOINT_LABEL,num_channels=config.NUM_CHANNELS)\n",
    "demand = Demand(demand_data=demand_data)\n",
    "rwa = RWA(tpg.gen_channel_names(config.NUM_CHANNELS), config.NUM_K_PATHS)\n",
    "scheduler = SRPT(network, rwa, slot_size=config.SLOT_SIZE)\n",
    "env = DCN(network, demand, scheduler, slot_size=config.SLOT_SIZE, max_flows=config.MAX_FLOWS, max_time=config.MAX_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run simulation\n",
    "for episode in range(config.NUM_EPISODES):\n",
    "    print('\\nEpisode {}/{}'.format(episode+1,config.NUM_EPISODES))\n",
    "    observation = env.reset(config.LOAD_DEMANDS)\n",
    "    while True:\n",
    "        print('Time: {}'.format(env.curr_time))\n",
    "        action = scheduler.get_action(observation)\n",
    "        print('Action:\\n{}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Episode finished.')\n",
    "            env.get_scheduling_session_summary(print_summary=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.get_scheduling_session_summary(print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand config file imported.\n",
      "\n",
      "Episode 1/1\n",
      "Time: 10000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_4', 'size': 11111.0, 'src': 'server_0', 'dst': 'server_2', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_0', 'server_2'], 'channel': 'channel_1', 'packets': array([3000., 3000., 3000., 3000.]), 'time_arrived': 2979.0, 'time_completed': None, 'k_shortest_paths': [['server_0', 'server_2']]}, {'flow_id': 'flow_1', 'size': 1476.0, 'src': 'server_2', 'dst': 'server_3', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_2', 'server_1', 'server_3'], 'channel': 'channel_1', 'packets': array([3000.]), 'time_arrived': 302.0, 'time_completed': None, 'k_shortest_paths': [['server_2', 'server_1', 'server_3']]}]}\n",
      "Time: 20000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_2', 'size': 7866.0, 'src': 'server_2', 'dst': 'server_3', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_2', 'server_1', 'server_3'], 'channel': 'channel_1', 'packets': array([3000., 3000., 3000.]), 'time_arrived': 1068.0, 'time_completed': None, 'k_shortest_paths': [['server_2', 'server_1', 'server_3']]}, {'flow_id': 'flow_0', 'size': 4521.0, 'src': 'server_4', 'dst': 'server_0', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_4', 'server_2', 'server_0'], 'channel': 'channel_1', 'packets': array([3000., 3000.]), 'time_arrived': 0.0, 'time_completed': None, 'k_shortest_paths': [['server_4', 'server_2', 'server_0']]}]}\n",
      "Time: 30000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_6', 'size': 6836.0, 'src': 'server_2', 'dst': 'server_3', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_2', 'server_1', 'server_3'], 'channel': 'channel_1', 'packets': array([3000., 3000., 3000.]), 'time_arrived': 18866.0, 'time_completed': None, 'k_shortest_paths': [['server_2', 'server_1', 'server_3']]}]}\n",
      "Time: 40000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_7', 'size': 5089.0, 'src': 'server_1', 'dst': 'server_2', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_1', 'server_2'], 'channel': 'channel_1', 'packets': array([3000., 3000.]), 'time_arrived': 32839.0, 'time_completed': None, 'k_shortest_paths': [['server_1', 'server_2']]}]}\n",
      "Time: 50000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_8', 'size': 12314.0, 'src': 'server_1', 'dst': 'server_2', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_1', 'server_2'], 'channel': 'channel_1', 'packets': array([3000., 3000., 3000., 3000., 3000.]), 'time_arrived': 33370.0, 'time_completed': None, 'k_shortest_paths': [['server_1', 'server_2']]}]}\n",
      "Time: 60000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_5', 'size': 1030.0, 'src': 'server_3', 'dst': 'server_2', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_3', 'server_1', 'server_2'], 'channel': 'channel_1', 'packets': array([3000.]), 'time_arrived': 18760.0, 'time_completed': None, 'k_shortest_paths': [['server_3', 'server_1', 'server_2']]}]}\n",
      "Time: 70000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_9', 'size': 6261.0, 'src': 'server_3', 'dst': 'server_2', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_3', 'server_1', 'server_2'], 'channel': 'channel_1', 'packets': array([3000., 3000., 3000.]), 'time_arrived': 33584.0, 'time_completed': None, 'k_shortest_paths': [['server_3', 'server_1', 'server_2']]}]}\n",
      "Time: 80000.0\n",
      "Action:\n",
      "{'chosen_flows': [{'flow_id': 'flow_3', 'size': 4812.0, 'src': 'server_4', 'dst': 'server_1', 'establish': 1, 'parent_deps': None, 'completed_parent_deps': [], 'child_deps': None, 'parent_op_run_time': None, 'time_parent_op_started': None, 'parent_op': None, 'dependency_type': None, 'child_op': None, 'can_schedule': 1, 'job_id': None, 'path': ['server_4', 'server_2', 'server_1'], 'channel': 'channel_1', 'packets': array([3000., 3000.]), 'time_arrived': 2956.0, 'time_completed': None, 'k_shortest_paths': [['server_4', 'server_2', 'server_1']]}]}\n",
      "Episode finished.\n"
     ]
    }
   ],
   "source": [
    "import trafpy.generator as tpg\n",
    "from trafpy.manager import Demand, RWA, SRPT, DCN\n",
    "import config\n",
    "\n",
    "# create demand data\n",
    "flow_size_dist = tpg.gen_named_val_dist(dist='weibull',\n",
    "                                        params={'_alpha': 1.4, '_lambda': 7000},\n",
    "                                        round_to_nearest=1)\n",
    "interarrival_time_dist = tpg.gen_named_val_dist(dist='lognormal',\n",
    "                                                params={'_mu': 7.4, '_sigma': 2},\n",
    "                                                round_to_nearest=1)\n",
    "network = tpg.gen_simple_network(ep_label=config.ENDPOINT_LABEL,\n",
    "                                 num_channels=config.NUM_CHANNELS)\n",
    "node_dist = tpg.gen_multimodal_node_dist(eps=network.graph['endpoints'],\n",
    "                                         num_skewed_nodes=1)\n",
    "flow_centric_demand_data = tpg.create_demand_data(num_demands=config.NUM_DEMANDS,\n",
    "                                                  eps=network.graph['endpoints'],\n",
    "                                                  node_dist=node_dist,\n",
    "                                                  flow_size_dist=flow_size_dist,\n",
    "                                                  interarrival_time_dist=interarrival_time_dist)\n",
    "\n",
    "# init manager\n",
    "demand = Demand(demand_data=flow_centric_demand_data)\n",
    "rwa = RWA(tpg.gen_channel_names(config.NUM_CHANNELS),\n",
    "          config.NUM_K_PATHS)\n",
    "scheduler = SRPT(network, rwa, slot_size=config.SLOT_SIZE)\n",
    "env = DCN(network,\n",
    "          demand,\n",
    "          scheduler,\n",
    "          slot_size=config.SLOT_SIZE,\n",
    "          max_flows=config.MAX_FLOWS,\n",
    "          max_time=config.MAX_TIME)\n",
    "\n",
    "# run simulation\n",
    "for episode in range(config.NUM_EPISODES):\n",
    "    print('\\nEpisode {}/{}'.format(episode+1,config.NUM_EPISODES))\n",
    "    observation = env.reset(config.LOAD_DEMANDS)\n",
    "    while True:\n",
    "        print('Time: {}'.format(env.curr_time))\n",
    "        action = scheduler.get_action(observation)\n",
    "        print('Action:\\n{}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Episode finished.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
