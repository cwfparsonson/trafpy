{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Flow-Centric DCN Benchmark Traffic\n",
    "\n",
    "In this example, we will use `TrafPy` to generate the flow-centric data centre network benchmark traffic traces (University, Private Enterprise, Commercial Cloud, and Social Media Cloud) for a custom network topology. We will then save these traffic traces as .pickle files, which can then be imported into any simulation, emulation, or experimentation environment/test bed independently of `TrafPy`. We will also organise the demonstrated traffic into time slots and generate an sqlite data base which we can save to our disk and access during a simulation, thereby enabling us to scale to very large simulation sizes.\n",
    "\n",
    "To do this, we will need to use the `trafpy.generator` and `trafpy.benchmarker` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "from trafpy.benchmarker import BenchmarkImporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Network\n",
    "\n",
    "Consider that you have a network with $n_{n}=64$ end points/machines/leaf nodes, and that you've defined your network externally to TrafPy, but you would like to use TrafPy to generate the University, Private Enterprise, Commercial Cloud, and Social Media Cloud traffic traces for your network. Regardless of your network's topology, we can use the `trafpy.generator.gen_arbitrary_network()` function to initialise an arbitrary representation of your network which TrafPy can recognise and create traffic data for.\n",
    "\n",
    "Start by setting how many end points are in your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) End Point Clusters/Racks\n",
    "In the context of data centre networks, it is common to cluster end points/machines into *racks*. This can lead to varying levels of end point and end point pair traffic loads (e.g. inter vs. intra-rack traffic load). `TrafPy` lets you group your end points into racks by providing a `racks_dict` dictionary which maps racks (keys) to the list of end points (values) within each rack. If your network did not have any clustering/racks, you could simply set `racks_dict=None`, but consider that your $n_{n}=64$ network has 8 servers per rack (8 racks overall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 1, 2, 3, 4, 5, 6, 7], 1: [8, 9, 10, 11, 12, 13, 14, 15], 2: [16, 17, 18, 19, 20, 21, 22, 23], 3: [24, 25, 26, 27, 28, 29, 30, 31], 4: [32, 33, 34, 35, 36, 37, 38, 39], 5: [40, 41, 42, 43, 44, 45, 46, 47], 6: [48, 49, 50, 51, 52, 53, 54, 55], 7: [56, 57, 58, 59, 60, 61, 62, 63]}\n"
     ]
    }
   ],
   "source": [
    "racks_dict, num_racks = {}, 8\n",
    "eps_per_rack = int(num_eps/num_racks)\n",
    "for rack in range(num_racks):\n",
    "    racks_dict[rack] = [ep for ep in range(rack*eps_per_rack, (rack*eps_per_rack)+eps_per_rack)]\n",
    "print(racks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Bandwidth Capacity\n",
    "\n",
    "Each link in your network will have (1) some bandwidth capacity per link channel (the maximum rate of information transfer per unit time on each channel) $C_{c}$, and (2) some number of channels per link $n_{c}$. `TrafPy` will assume that your links are *bidirectional*, meaning traffic can simultaneously be outbound and inbound from/to a given end point, therefore each link channel's bandwidth is split between the end points' transceiver (source) and receiver (destination) port. Consider that your network has link channel capacity $C_{c}=1250$ and $n_{c}=1$ channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_capacity = 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now initialise your network in a format recognised by `TrafPy` using the `trafpy.generator.gen_arbitrary_network()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tpg.gen_arbitrary_network(num_eps=num_eps, ep_capacity=ep_capacity, racks_dict=racks_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *total network capacity* $C_{t}$ is the maximum amount of information which can be transmitted through the entire network per unit time. If 2 end points are connected, the most amount of information they can transfer along a single channel per unit time is the end point bandwidth capacity per channel $C_{c}$ (*not* $2 \\cdot C_{c}$), therefore the network's total capacity $C_{t}$ can be evaluated as:\n",
    "\n",
    "\\begin{equation}\n",
    "    C_{t} = \\frac{n_{n} \\cdot C_{c} \\cdot n_{c}}{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `TrafPy` networks and graphs are `NetworkX` graphs (see [here](https://networkx.org/documentation/stable/tutorial.html) for details) which can be treated as Python dictionaries. All `TrafPy` networks have the following 'global' graph keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['endpoints', 'endpoint_label', 'num_channels_per_link', 'ep_link_capacity', 'ep_link_port_capacity', 'max_nw_capacity', 'curr_nw_capacity_used', 'num_active_connections', 'total_connections_blocked', 'node_labels', 'topology_type', 'channel_names', 'rack_to_ep_dict', 'ep_to_rack_dict'])\n",
      "arbitrary_endpoints_64_chancap_1250_channels_1\n"
     ]
    }
   ],
   "source": [
    "print(net.graph.keys())\n",
    "print(net.graph['topology_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that TrafPy has automatically evaluated the maximum capacity of your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000.0\n",
      "arbitrary_endpoints_64_chancap_1250_channels_1\n"
     ]
    }
   ],
   "source": [
    "print(net.graph['max_nw_capacity'])\n",
    "print(net.graph['topology_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also that all references to capacity have been unitless. $C_{c}=1250$ could refer to e.g. 1250 B/s, 1250 $\\mu$B/ms, 1250 Gbps, and so on. This is intentional, and allows you to plug in whatever values you find convenient. Just ensure the information and time units you use are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Load Configuration\n",
    "\n",
    "The network load refers to the overall amount of traffic received by the network. This is commonly referred to as a load rate (information units arriving per unit time) or as a load fraction (the fraction of the total network capacity being requested for a given duration). `TrafPy` typically uses the load fraction definition for load, therefore loads can be varied between 0 and 1.\n",
    "\n",
    "A key feature of `TrafPy` is that you can generate any load for your custom network. To do this, you should provide `TrafPy` with a `network_load_config` dictionary which tells `TrafPy` (1) the end point capacity of your network, (2) the maximum capacity of your network, and (3) the overall load fraction you would like `TrafPy` to generate for your network. Consider that you would like `TrafPy` to generate a 0.1 load traffic trace for your network (i.e. around 10% of your total network capacity will be requested per unit time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_load_config = {'network_rate_capacity': net.graph['max_nw_capacity'], \n",
    "                       'ep_link_capacity': net.graph['ep_link_capacity'],\n",
    "                       'target_load_fraction': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the TrafPy Benchmarks\n",
    "\n",
    "The `trafpy.benchmarker.BenchmarkImporter` can be used to load pre-defined `TrafPy` benchmarks which are compatible with the end points we previously defined for our custom network.\n",
    "\n",
    "`TrafPy` is able to generate arbitrary distributions and therefore arbitrary traffic traces. As the nature of applications handled by real data centres changes, so too will the resultant traffic traces. Therefore, the initial benchmarks established for `TrafPy` have been stored under `benchmark_version='v001'` with the anticipation that future benchmark versions will be established with new and evolving traffic traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_prev_dist=False. Will re-generate dists with given network params and override any previously saved distributions.\n"
     ]
    }
   ],
   "source": [
    "importer = BenchmarkImporter(benchmark_version='v001', load_prev_dists=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the University, Private Enterprise, Commercial Cloud, and Social Media Cloud `TrafPy` benchmark distributions adapted for our network's endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_benchmark_dists() got an unexpected keyword argument 'rack_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b8aaa64aebe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     dcn_dists[dcn] = importer.get_benchmark_dists(benchmark_name=dcn, \n\u001b[1;32m      4\u001b[0m                                                   \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                   rack_dict=net.graph['rack_to_ep_dict'])\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_benchmark_dists() got an unexpected keyword argument 'rack_dict'"
     ]
    }
   ],
   "source": [
    "dcn_dists = {}\n",
    "for dcn in ['university', 'private_enterprise', 'commercial_cloud', 'social_media_cloud']:\n",
    "    dcn_dists[dcn] = importer.get_benchmark_dists(benchmark_name=dcn, \n",
    "                                                  eps=net.graph['endpoints'], \n",
    "                                                  racks_dict=net.graph['rack_to_ep_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each benchmark contains the following distributions (stored as dictionaries with key (distribution name) value (distribution hash table) pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(dcn_dists['university'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For flow-centric traffic, we only need to worry about flow size, inter-arrival time, and node distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Optional) Define Additional Trace Parameters\n",
    "\n",
    "`TrafPy` generates traffic trace data for your network using the `trafpy.generator.create_demand_data()` function. This function has some useful additional parameters you can define to further customise the trace generated for your network (see documentation for details).\n",
    "\n",
    "An important parameter in `TrafPy` traffic generation is the `jensen_shannon_distance_threshold`. Since `TrafPy` traffic data are generated by sampling from some pre-defined distributions, how similar the sampled data are to the original distributions will depend on the number of demands generated/sampled by the law of large numbers. `TrafPy` lets you set the `jensen_shannon_distance_threshold`, which must be between 0 and 1. A lower value will result in more demands being generated and therefore a data set more similar to the original distributions (in the example, benchmarks) you are sampling from, whereas a higher value will be less reliably similar but will result in fewer demands (which can reduce the memory and time size of your tests).\n",
    "\n",
    "Other useful additional parameters include `min_last_demand_arrival_time`, `min_num_demands`, and `max_num_demands`. See documentation for details.\n",
    "\n",
    "Here, we will set `jensen_shannon_distance_threshold=0.9` (a high value so that we don't generate many demands and therefore save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_threshold = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate the Traffic Traces\n",
    "\n",
    "With our network defined and our benchmark distributions loaded, we are now ready to generate some traffic for our custom network using the `trafpy.generator.create_demand_data()` function. We will save our traffic into separate .pickle files. These files can then be imported into your own simulation, emulation, and/or experimentation test beds to test your own systems under these `TrafPy` benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "path_to_data = 'data/generate_flow_benchmark_traffic/'\n",
    "Path(path_to_data).mkdir(exist_ok=True, parents=True)\n",
    "for dcn in dcn_dists.keys():\n",
    "    print('Generating \\'{}\\' traffic demands for {} network'.format(dcn, net.graph['topology_type']))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # get node, flow size, and flow inter-arrival time benchmark dists\n",
    "    dists = dcn_dists[dcn]\n",
    "    \n",
    "    # generate traffic demands\n",
    "    demand_data = tpg.create_demand_data(eps=net.graph['endpoints'],\n",
    "                                         node_dist=dists['node_dist'],\n",
    "                                         flow_size_dist=dists['flow_size_dist'],\n",
    "                                         interarrival_time_dist=dists['interarrival_time_dist'],\n",
    "                                         network_load_config=network_load_config,\n",
    "                                         jensen_shannon_distance_threshold=jsd_threshold)\n",
    "    \n",
    "    # save demands as pickle file\n",
    "    filename = path_to_data+'{}_demand_data.pickle'.format(dcn)\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(demand_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`demand_data` is a dictionary storing the following information for each flow (where the values are a list of values corresponding to the values assigned to each flow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demand_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Analyse the Generated Traffic\n",
    "\n",
    "At this point, you could do your own analysis of the traffic you've generated by loading the saved data into your own scripts. However, `TrafPy` provides some useful tools for this.\n",
    "\n",
    "We can encode our saved `demand_data` files as `trafpy.generator.Demand()` objects, and then use the `trafpy.generator.DemandsAnalyser` and `trafpy.generator.DemandPlotter` objects to analyse them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafpy.generator import Demand, DemandsAnalyser, DemandPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First collect the demand objects from each demand_data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect demand objects\n",
    "demands = {}\n",
    "for dcn in dcn_dists.keys():\n",
    "    with gzip.open(path_to_data+'{}_demand_data.pickle'.format(dcn), 'rb') as f:\n",
    "        demand_data = pickle.load(f)\n",
    "    demands[dcn] = Demand(demand_data, net.graph['endpoints'], name=dcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `trafpy.generator.DemandsAnalyser()` to print a summary table of all the demand data sets you generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary table\n",
    "analyser = DemandsAnalyser(*list(demands.values()), jobcentric=False)\n",
    "analyser.compute_metrics(print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualise your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise distributions\n",
    "for dcn, demand in demands.items():\n",
    "    print(dcn)\n",
    "    plotter = DemandPlotter(demand)\n",
    "    plotter.plot_flow_size_dist(logscale=True, figsize=(12,6))\n",
    "    plotter.plot_interarrival_time_dist(logscale=True, figsize=(12,6))\n",
    "    plotter.plot_node_dist(eps=net.graph['endpoints'],\n",
    "                           chord_edge_width_range=[1,25],\n",
    "                           chord_edge_display_threshold=0.005)\n",
    "    plotter.plot_node_load_dists(eps=net.graph['endpoints'], \n",
    "                                 ep_link_bandwidth=net.graph['ep_link_capacity'],\n",
    "                                 plot_extras=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. (Optional) Generate a slots dict data base\n",
    "\n",
    "\n",
    "Many network experiments are based on time slots. I.e. during a time slot of e.g. 10 time units, some number of flows arrive. The `trafpy.generator.Demand()` class has a useful `get_slots_dict()` method to automatically organise your generated traffic demands into time slots given the `slot_size` you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots_dict = demand.get_slots_dict(slot_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `slots_dict` dictionary contains indices 0-n for `n` slots, as well as some other useful information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slots_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. To access the flows which arrived in the first time slot (with upper bound and lower bound times on the time slot also given since this is often useful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slots_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time slot flows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slots_dict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on.\n",
    "\n",
    "For large simulations, it is recommended to save the `slots_dict` as a database on your disk which you can query during your simulation. The `SqliteDict` library is particularly useful for this since it lets you save a database in .sqlite file format whilst still allowing you to query the database as if it were a normal Python dictionary. See [here](https://pypi.org/project/sqlitedict/) for more details.\n",
    "\n",
    "To save your `slots_dict` as a .sqlite database with `SqliteDict`, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlitedict import SqliteDict\n",
    "import json\n",
    "\n",
    "with SqliteDict(path_to_data+'{}_demand_data_slots_dict.sqlite'.format(dcn)) as _slots_dict:\n",
    "    for key, val in slots_dict.items():\n",
    "        if type(key) is not str:\n",
    "            _slots_dict[json.dumps(key)] = val\n",
    "        else:\n",
    "            _slots_dict[key] = val\n",
    "    _slots_dict.commit()\n",
    "    _slots_dict.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
