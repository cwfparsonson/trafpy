{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Multiple Benchmark Flow Traffic Sets\n",
    "\n",
    "In this example, we write a script which will generate multiple benchmark traffic sets in a loop and save them in .pickle format. We will assume we are generating traffic for a `TrafPy` fat tree topology, although of course you can generate traffic for any arbitrary topology defined outside of `TrafPy` (see documentation and other examples).\n",
    "\n",
    "We will generate the rack distribution sensitivity benchmark data set for loads 0.1-0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafpy.generator as tpg\n",
    "from trafpy.benchmarker import BenchmarkImporter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict # use for initialising arbitrary length nested dict\n",
    "from sqlitedict import SqliteDict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Generation Configuration\n",
    "\n",
    "If you were writing this in a script rather than a Jupyter Notebook, you may want to e.g. put this next cell in a `config.py` file and import the file into a separate script for conciseness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# general configuration\n",
    "# -------------------------------------------------------------------------\n",
    "# define benchmark version\n",
    "BENCHMARK_VERSION = 'v001'\n",
    "\n",
    "# define minimum number of demands to generate (may generate more to meet jensen_shannon_distance_threshold and/or min_last_demand_arrival_time)\n",
    "MIN_NUM_DEMANDS = None\n",
    "MAX_NUM_DEMANDS = 5000 \n",
    "\n",
    "# define maximum allowed Jenson-Shannon distance for flow size and interarrival time distributions (lower value -> distributions must be more similar -> higher number of demands will be generated) (must be between 0 and 1)\n",
    "JENSEN_SHANNON_DISTANCE_THRESHOLD = 0.3\n",
    "\n",
    "# define minimum time of last demand's arrival (helps define minimum simulation time)\n",
    "MIN_LAST_DEMAND_ARRIVAL_TIME = None \n",
    "\n",
    "# define network load fractions\n",
    "LOADS = [round(load, 3) for load in np.arange(0.1, 0.4, 0.1).tolist()] # ensure no python floating point arithmetic errors\n",
    "\n",
    "# define whether or not to TrafPy packer should auto correct invalid node distribution(s)\n",
    "AUTO_NODE_DIST_CORRECTION = True\n",
    "\n",
    "# slot size (if None, won't generate slots_dict database)\n",
    "# SLOT_SIZE = None \n",
    "SLOT_SIZE = 1000.0 # 50.0 1000.0 10.0\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# benchmark-specific configuration\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "BENCHMARKS = ['rack_sensitivity_0',\n",
    "              'rack_sensitivity_02',\n",
    "              'rack_sensitivity_04',\n",
    "              'rack_sensitivity_06',\n",
    "              'rack_sensitivity_08']\n",
    "\n",
    "# define network topology for each benchmark\n",
    "net = tpg.gen_fat_tree(k=4, \n",
    "                       L=2, \n",
    "                       n=8, \n",
    "                       num_channels=1, \n",
    "                       server_to_rack_channel_capacity=1250, # 1250\n",
    "                       rack_to_edge_channel_capacity=1000, \n",
    "                       edge_to_agg_channel_capacity=1000, \n",
    "                       agg_to_core_channel_capacity=2000)\n",
    "NETS = {benchmark: net for benchmark in BENCHMARKS}\n",
    "\n",
    "# define network capacity for each benchmark\n",
    "NETWORK_CAPACITIES = {benchmark: net.graph['max_nw_capacity'] for benchmark in BENCHMARKS}\n",
    "NETWORK_EP_LINK_CAPACITIES = {benchmark: net.graph['ep_link_capacity'] for benchmark in BENCHMARKS}\n",
    "\n",
    "# define network racks for each benchmark\n",
    "RACKS_DICTS = {benchmark: net.graph['rack_to_ep_dict'] for benchmark in BENCHMARKS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a Function to Generate the Benchmark Traffic\n",
    "\n",
    "This function should use the above configuration variables to generate traffic for each of our benchmarks as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_benchmark_demands(path_to_save=None, \n",
    "                          load_prev_dists=True,\n",
    "                          overwrite=False):\n",
    "    '''\n",
    "    If slot size is not None, will also generate an sqlite database for the slots_dict\n",
    "    dictionary. This is useful if later during simulations want to have pre-computed\n",
    "    slots_dict rather than computing & storing them in memory.\n",
    "\n",
    "    '''\n",
    "    if path_to_save[-1] == '/' or path_to_save[-1] == '\\\\':\n",
    "        path_to_save = path_to_save[:-1]\n",
    "\n",
    "    # init benchmark importer\n",
    "    importer = BenchmarkImporter(BENCHMARK_VERSION, load_prev_dists=load_prev_dists)\n",
    "\n",
    "    # load distributions for each benchmark\n",
    "    benchmark_dists = {benchmark: {} for benchmark in BENCHMARKS}\n",
    "\n",
    "    nested_dict = lambda: defaultdict(nested_dict)\n",
    "    benchmark_demands = nested_dict()\n",
    "\n",
    "    # begin generating data for each benchmark\n",
    "    num_loads = len(LOADS)\n",
    "    start_loops = time.time()\n",
    "    print('\\n~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*')\n",
    "    print('Benchmarks to Generate: {}'.format(BENCHMARKS))\n",
    "    print('Loads to generate: {}'.format(LOADS))\n",
    "    for benchmark in BENCHMARKS:\n",
    "        print('~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*')\n",
    "        print('Generating demands for benchmark \\'{}\\'...'.format(benchmark))\n",
    "        \n",
    "        # get racks and endpoints\n",
    "        racks_dict = RACKS_DICTS[benchmark]\n",
    "        if racks_dict is not None:\n",
    "            eps_racks_list = [eps for eps in racks_dict.values()]\n",
    "            eps = []\n",
    "            for rack in eps_racks_list:\n",
    "                for ep in rack:\n",
    "                    eps.append(ep)\n",
    "        else:\n",
    "            eps = NETS[benchmark].graph['endpoints']\n",
    "\n",
    "        start_benchmark = time.time()\n",
    "        load_counter = 1\n",
    "        benchmark_dists[benchmark] = importer.get_benchmark_dists(benchmark, eps, racks_dict=racks_dict)\n",
    "        for load in LOADS:\n",
    "            start_load = time.time()\n",
    "            network_load_config = {'network_rate_capacity': NETWORK_CAPACITIES[benchmark], \n",
    "                                   'ep_link_capacity': NETWORK_EP_LINK_CAPACITIES[benchmark],\n",
    "                                   'target_load_fraction': load,\n",
    "                                   'disable_timeouts': True}\n",
    "            print('Generating demand data for benchmark {} load {}...'.format(benchmark, load))\n",
    "            if benchmark_dists[benchmark]['num_ops_dist'] is not None:\n",
    "                # job-centric\n",
    "                use_multiprocessing = True\n",
    "            else:\n",
    "                # flow-centric\n",
    "                use_multiprocessing = False\n",
    "            demand_data = tpg.create_demand_data(min_num_demands=MIN_NUM_DEMANDS,\n",
    "                                             max_num_demands=MAX_NUM_DEMANDS, \n",
    "                                             eps=eps,\n",
    "                                             node_dist=benchmark_dists[benchmark]['node_dist'],\n",
    "                                             flow_size_dist=benchmark_dists[benchmark]['flow_size_dist'],\n",
    "                                             interarrival_time_dist=benchmark_dists[benchmark]['interarrival_time_dist'],\n",
    "                                             num_ops_dist=benchmark_dists[benchmark]['num_ops_dist'],\n",
    "                                             c=3,\n",
    "                                             jensen_shannon_distance_threshold=JENSEN_SHANNON_DISTANCE_THRESHOLD,\n",
    "                                             network_load_config=network_load_config,\n",
    "                                             min_last_demand_arrival_time=MIN_LAST_DEMAND_ARRIVAL_TIME,\n",
    "                                             auto_node_dist_correction=AUTO_NODE_DIST_CORRECTION,\n",
    "                                             use_multiprocessing=use_multiprocessing,\n",
    "                                             print_data=False)\n",
    "\n",
    "            file_path = path_to_save + '/benchmark_{}_load_{}'.format(benchmark, load)\n",
    "            tpg.pickle_data(path_to_save=file_path, data=demand_data, overwrite=overwrite)\n",
    "\n",
    "            # reset benchmark demands dict to save memory\n",
    "            benchmark_demands = nested_dict()\n",
    "\n",
    "            if SLOT_SIZE is not None:\n",
    "                # generate slots dict and save as database\n",
    "                print('Creating slots_dict database with slot_size {}...'.format(SLOT_SIZE))\n",
    "                s = time.time()\n",
    "                demand = tpg.Demand(demand_data, eps=eps)\n",
    "                with SqliteDict(file_path+'_slotsize_{}_slots_dict.sqlite'.format(SLOT_SIZE)) as slots_dict:\n",
    "                    for key, val in demand.get_slots_dict(slot_size=SLOT_SIZE, include_empty_slots=True, print_info=True).items():\n",
    "                        if type(key) is not str:\n",
    "                            slots_dict[json.dumps(key)] = val\n",
    "                        else:\n",
    "                            slots_dict[key] = val\n",
    "                    slots_dict.commit()\n",
    "                    slots_dict.close()\n",
    "                e = time.time()\n",
    "                print('Created slots_dict database in {} s'.format(e-s))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            end_load = time.time()\n",
    "            print('Generated \\'{}\\' demands for load {} of {} in {} seconds.'.format(benchmark, load_counter, num_loads, end_load-start_load))\n",
    "            load_counter += 1\n",
    "\n",
    "        end_benchmark = time.time()\n",
    "        print('Generated demands for benchmark \\'{}\\' in {} seconds.'.format(benchmark, end_benchmark-start_benchmark))\n",
    "\n",
    "    end_loops = time.time()\n",
    "    print('Generated all benchmarks in {} seconds.'.format(end_loops-start_loops))\n",
    "\n",
    "    return benchmark_demands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate the Benchmark Traffic\n",
    "\n",
    "We will generate each of our traffic sets 2x to enable us to run 2 repeat experiments for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_prev_dist=False. Will re-generate dists with given network params and override any previously saved distributions.\n",
      "\n",
      "~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\n",
      "Benchmarks to Generate: ['rack_sensitivity_0', 'rack_sensitivity_02', 'rack_sensitivity_04', 'rack_sensitivity_06', 'rack_sensitivity_08']\n",
      "Loads to generate: [0.1, 0.2, 0.3, 0.4]\n",
      "~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\n",
      "Generating demands for benchmark 'rack_sensitivity_0'...\n",
      "Set to save benchmark rack_sensitivity_0 distribution data to /home/cwfparsonson/Insync/zciccwf@ucl.ac.uk/OneDriveBiz/ipes_cdt/phd_project/projects/trafpy/trafpy/benchmarker/versions/benchmark_v001/data/rack_sensitivity_0/\n",
      "Saved node_dist distribution data to /home/cwfparsonson/Insync/zciccwf@ucl.ac.uk/OneDriveBiz/ipes_cdt/phd_project/projects/trafpy/trafpy/benchmarker/versions/benchmark_v001/data/rack_sensitivity_0/\n",
      "Saved flow_size_dist distribution data to /home/cwfparsonson/Insync/zciccwf@ucl.ac.uk/OneDriveBiz/ipes_cdt/phd_project/projects/trafpy/trafpy/benchmarker/versions/benchmark_v001/data/rack_sensitivity_0/\n",
      "Saved interarrival_time_dist distribution data to /home/cwfparsonson/Insync/zciccwf@ucl.ac.uk/OneDriveBiz/ipes_cdt/phd_project/projects/trafpy/trafpy/benchmarker/versions/benchmark_v001/data/rack_sensitivity_0/\n",
      "Generating demand data for benchmark rack_sensitivity_0 load 0.1...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'num_ops_dist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fdac14203d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     benchmark_demands = gen_benchmark_demands(path_to_save=path_to_save,\n\u001b[1;32m      5\u001b[0m                                               \u001b[0mload_prev_dists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                               overwrite=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-dfdb00b4214d>\u001b[0m in \u001b[0;36mgen_benchmark_demands\u001b[0;34m(path_to_save, load_prev_dists, overwrite)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                              \u001b[0mflow_size_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbenchmark_dists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow_size_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                              \u001b[0minterarrival_time_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbenchmark_dists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interarrival_time_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                                              \u001b[0mnum_ops_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbenchmark_dists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_ops_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                                              \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                              \u001b[0mjensen_shannon_distance_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJENSEN_SHANNON_DISTANCE_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_ops_dist'"
     ]
    }
   ],
   "source": [
    "for _set in range(2):\n",
    "    path_to_save = '../data/generate_multiple_benchmark_traffic_sets/set_{}_benchmark_data'.format(_set)\n",
    "    Path(path_to_save).mkdir(exist_ok=True, parents=True)\n",
    "    benchmark_demands = gen_benchmark_demands(path_to_save=path_to_save,\n",
    "                                              load_prev_dists=False,\n",
    "                                              overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_scheduler",
   "language": "python",
   "name": "deep_scheduler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
